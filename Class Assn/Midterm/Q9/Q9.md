# Question 9

## Part A

I think that a large part of robotics is technical, but there are definitely a lot of places 
where you get in muddy water ethically. If you aren't careful, you can start doing some really 
unethical things. We think that it's dangerous to consider robotics as purely technical, and 
that anyone involved in robotics should have some degree of ethical training/thinking.

## Part B

Isaac Asimov's 3 laws of robotics (simplified) are "do not harm human beings, whether through 
inaction or action", "obey human beings unless the order breaks the 1st law" and "preserve your 
own existence unless doing so breaks the 1st or 2nd law". Algorithmically this is super 
difficult to implement, because you have to get a computer to understand what "injuring a human 
being" entails. You also have to build adequate communication systems so that the robot can 
process human orders whether through speech or otherwise. You also have to build a complex 
hirerarchy to be able to determine whether you protect your own existence, obey a human order 
or prevent harm to a human. There are so many different possibilities here that make it 
impossible to obey these 3 laws without having artificial intelligence that is far in the 
future.

However, you can build a simple hirerachy that will obey these laws partially - the robot may 
not be able to prevent all kinds of harm but you can program it with reasonable awareness of 
its surroundings. It also may not be able to respond to all types of commands but can have a 
purpose-built interface. It can also take reasonable measures to prevent known kinds of harm to 
itself.

The scenario we are choosing is a robotic train. Here is some basic pseudocode.

```
# if obstruction in path
    # brake at a reasonable speed to avoid collision
    # only brake up to a maximum deceleration - this is a precoded value (don't brake too hard 
    # as to derail the train, but you can brake hard enough to permanently damage the brakes)
# else
    # if human says go
        # go, as long as speed is not above maximum safe speed
        # if going too fast would damage the train (but not harm people onboard), allow it but 
        # warn the user
    # if human says stop
        # stop, as long as deceleration is not above maximum safe deceleration
        # if braking too fast would permanently damage the brakes (but not harm people 
        # onboard), allow it but warn the user
    # if human says nothing
        # maintain speed, or slow down if the speed is above what is good for the train
```

This scenario prioritizes people's lives first, then human orders, and only then the wellbeing 
of the train itself.

## Part 3

I agree with both articles that the liability should rest primarily on the self-driving car 
manufacturer and not with the backup driver - but I do think that the backup driver has partial 
responsibility to make sure their car is in good working order. I think that if the car isn't 
in good working order and the backup driver is aware of that, then they should possibly be held 
liable. I think it would be good to have safety certifications that self-driving cars are 
required to pass to be able to be street-legal. Having a centralized safety certification would 
ensure that the requirements and expectations are clear, and shift blame off of the backup 
driver and onto the product manufacturer.

## Part 4

I think that laws in some cases (e.g. self driving cars above) are necessary for safety. I do 
think that too much regulation could become a big problem, with companies (or individuals) 
manufacturing robots being bound by a lot of red tape, and unable to try out new things because 
of laws. I think that too much regulation will especially harm individuals who are wanting to 
experiment with robotics.
